---
title: 'Blog Post number 3'
date: 2014-08-14
permalink: /posts/2014/08/blog-post-3/
tags:
  - cool posts
  - category1
  - category2
---

This is a sample blog post. Lorem ipsum I can't remember the rest of lorem ipsum and don't have an internet connection right now. Testing testing testing this blog post. Blog posts are cool. 

Deep generative models assume that data $x$ is generated from latent continuous variable $z$ through some random process. This process contains two consequent steps: (1) a value z is sampled from a prior distribution $p(z)$; (2) A observation $x$ is generated from the conditional distribution $p_{\theta}(x|z)$, which is parameterized through deep neural networks. Thus, it defines the joint distribution between the observation $$x$$ and the latent variable $z$,

$$p(x, z) = p(z)p_{\theta}(x|z)$$(1)

The process is hidden because the $z$ is unobserved. We learn this process by maximizing the marginal log-likelihood $p(x)$ over the parameter $\theta$ and latent variable $z$ from the training set. After the training, the $z$ can be viewed as the representation of $x$ because it captures the most relevant information of $x$. Thus, the learning process is also called representation learning.

Headings are cool
======

You can have many headings
======

Aren't headings cool?
------
