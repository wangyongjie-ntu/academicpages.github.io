---
title: 'Varitional Autoencoder'
date: 2021-04-21
permalink: /posts/2021/04/vae/
tags:
  - VAE
  - Machine Learning
  - Approximation Inference
---

Note of VAE. 

# Introduction
Deep generative models assume that data $x$ is generated from latent continuous variable $z$ through some random process. This process contains two consequent steps: (1) a value $z$ is sampled from a prior distribution $p(z)$; (2) A observation $x$  is generated from the conditional distribution $p_{\theta}(x\| z)$, which is parameterized through deep neural networks. Thus, it defines the joint distribution between the observation $x$ and the latent variable $z$,

$$p(x, z) = p(z)p_{\theta}(x|z)$$

The process is hidden because the $z$ is unobserved. We learn this process by maximizing the marginal log-likelihood $p(z)$ over the parameter $\theta$ and latent variable $z$ from the training set. After the training, the $z$ can be viewed as the representation of $x$ because it captures the most relevant information of $x$. Thus, the learning process is also called representation learning.

We shall call $\{X, Z\}$ the complete data set, and we shall refer to the actual observed data $X$ as incomplete data. The likelihood function for the complete data set is simply takes the form of $\ln p(X,Z\|\theta)$. In practice, we are not given the complete data set $\{X, Z\}$, but only the incomplete data $X$. Our state of knowledge of the values of the latent variables in $Z$ in given only by the posterior distribution $p(Z\|X,\theta)$. Because we cannot use the complete-data log likelihood, we consider instead its expected value under the posterior distribution of the latent variable $Z$.

# Definition
Consider a probabilistic model where all the observed variables are denoted by $X$ and all the hidden variables by $Z$. The joint distribution $p(X,Z\|\theta)$ is governed by as set of parameters denoted by $\theta$. Our goal is to maximize the likelihood function that is given by,

$$p(X|\theta) = \sum_{Z}p(X,Z|\theta)$$

We shall suppose that direct optimization of $p(X\|\theta)$ is difficult, but that optimization of the complete-data likelihood function $p(X, Z\|\theta)$ is significantly easier. Next we introduce a distribution $q(Z)$ defined over the latent variables, and we observe that, for any choice of $q(Z)$, the following decomposition holds,

$$\ln p(X|\theta) = \mathcal{L}(q, \theta)  + KL(q\|q)$$

where,

$$\mathcal{L}(q, \theta) =  \sum_{Z} q(Z) ln \{\frac{p(X,Z |\theta)}{q(Z)}\}$$

$$KL(q\|p) = - \sum_{Z} q(Z) ln \{\frac{p(Z|X, \theta)}{q(Z)}\} $$

**Proof**

$$\begin{aligned} KL(q\|p)  &= \sum_{Z} q(Z) \ln \frac{q(Z)}{p(Z|X)} = -\sum_{Z} q(Z) \ln\frac{p(Z|X)}{q(Z)} \\ &= -\sum_{Z} q(Z) \ln\frac{\frac{p(X,Z)}{p(X)}}{q(Z)} = -\sum_{Z} q(Z) \ln\frac{p(X,Z)}{q(Z)} \frac{1}{p(X)} \\ &=  \sum_{Z} q(Z) \ln\frac{p(X,Z)}{q(Z)} + \sum_{Z} q(Z) \ln p(X) \\ &= -\sum_{Z}q(Z) \ln \frac{p(X,Z)}{q(Z)} + \ln p(X)\end{aligned}$$

Because KL term is always greater than $0$. Therefore, $\mathcal{L}(q, \theta) \le \ln p(Z\|\theta)$, in other words that $\mathcal{L}(q, \theta)$ is a lower bound of  $\ln p(Z\|\theta)$.  If we want to maximize the $\ln p(Z\|\theta)$, we can maximize the lower bound.

$$\begin{aligned}\sum_{Z}q(Z) \ln \frac{p(X,Z)}{q(Z)}  &= \sum_{Z}q(Z) \ln \frac{p(X|Z)p(Z)}{q(Z)} \\ &= \sum_{Z}q(Z) \ln p(X|Z) + \sum_{Z}q(Z) \ln \frac{p(Z)}{q(Z)} \\ &=  \sum_{Z}q(Z) \ln p(X|Z) + KL(q(Z)\| p(Z)) \\ &= E_{q(Z)} log p(X|Z) + KL(q(Z)\| p(Z)) \end{aligned}$$

$q(Z)$ is the encoder function. $p(X\|Z)$ is the decoder function. 
