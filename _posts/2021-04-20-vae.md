---
title: 'Varitional Autoencoder'
date: 2021-04-21
permalink: /posts/2021/04/vae/
tags:
  - VAE
  - Machine Learning
  - Approximation Inference
---

Deep generative models assume that data $x$ is generated from latent continuous variable $z$ through some random process. This process contains two consequent steps: (1) a value z is sampled from a prior distribution $p(z)$; (2) A observation $x$ is generated from the conditional distribution $p_{\theta}(x|z)$, which is parameterized through deep neural networks. Thus, it defines the joint distribution between the observation $$x$$ and the latent variable $z$,

$$p(x, z) = p(z)p_{\theta}(x|z)$$(1)

The process is hidden because the $z$ is unobserved. We learn this process by maximizing the marginal log-likelihood $p(x)$ over the parameter $\theta$ and latent variable $z$ from the training set. After the training, the $z$ can be viewed as the representation of $x$ because it captures the most relevant information of $x$. Thus, the learning process is also called representation learning.

Headings are cool
======

------
